---
title: Stroke Clinical Predictive Model
author: "Tosin Dairo"
date: MAY 25, 2019
output: rmarkdown::github_document
categories: ["R"]
tags: ["Health", "Predictive_Models", "Machine_Learning"]
---

## Clinical Predictive Model classifier for stroke

Stroke-CPM is a tool that takes information available about a patient and their observed predictive factors, and makes a prediction regarding their diagnosis and causal factors.

+ Diagnosis – detects presence of stroke currently
+ Causal Factors – assess whether observed predictive factor(s) is/are likely to be contributors to the development of stroke presently or in the future

<br></br>

#### Observed Factors:

+ Gender
+ Age
+ Hypertension
+ Heart Disease
+ Ever Married
+ Work Type
+ Residence Type
+ Average Glucose Level
+ BMI
+ Smoking Status

<br></br> 



```{r echo=FALSE}
{{knitr::spin_child('init.R')}}
```



```{r include=FALSE}
getwd()
library(reticulate)
library(mice)
library(tidyverse)
library(MASS)
library(sjPlot)
library(dataPreparation)
library(DMwR)
library(randomForest)  
library(caret)
library(pROC)
library(ROCR)
library(ResourceSelection)
use_python("/Volumes/Loopdisk/Dev/PyDsc/env/bin/python3", required = TRUE)
# use_python("/usr/bin/python3", required = TRUE)
# use_virtualenv("~/.virtualenvs/r-reticulate", required = TRUE)
d <- read.csv("/Volumes/Loopdisk/Stroke-CPM/data/train_2v.csv")
reticulate::py_config()
```



```{python include=FALSE}
import matplotlib
matplotlib.use('TkAgg')
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```



```{r}
head(d)
```


#### Missing Values
```{r}
d$smoking_status <- na_if(d$smoking_status, "")
```



```{python}
print(r.d.isnull().sum())
fig, ax = plt.subplots(figsize=(14,12))
sns.heatmap(r.d.isnull(),yticklabels=False,cbar=False,cmap='viridis') 
```



```{r}
d$gender <- as.factor(d$gender)
d$ever_married <- as.factor(d$ever_married)
d$work_type <- as.factor(d$work_type)
d$Residence_type <- as.factor(d$Residence_type)
d$smoking_status <- as.factor(d$smoking_status)
str(d)
```


#### Multiple Imputation for Missing Data
> Using Predictive Mean Matching and Poly Regreession Technique

```{r}
imp <- mice(d, seed = 3333)
```



```{r}
imp$method
```



```{r}
dx <- mice::complete(imp)
```



```{python}
print(r.dx.isnull().sum())
fig, ax = plt.subplots(figsize=(14,12))
sns.heatmap(r.dx.isnull(),yticklabels=False,cbar=False,cmap='viridis') 
```


#### Exploratory Analysis

```{python}
fig, ax = plt.subplots(figsize=(14,12))
sns.pairplot(r.dx)
plt.show()
```



```{python}
sns.countplot(x='stroke', data = r.dx)
```



```{r echo=FALSE}
{{knitr::spin_child('label_encode.R')}}
```




```{r}
result <- encode.fit_transform(dx)

dx_ <- result[[2]]
# encode.transform()
```



```{r}
tab_df(table(dx_$stroke), title = "Stroke Class")
tab_df(table(dx_$hypertension), title = "Hypertension Class")
tab_df(table(dx_$heart_disease), title = "Heart Disease Class")
```



```{r}
# encoding <- build_encoding(dataSet = X_train, cols = "auto", verbose = TRUE)
# X_train <- one_hot_encoder(dataSet = X_train, encoding = encoding, drop = TRUE, verbose = TRUE)
# X_test <- one_hot_encoder(dataSet = X_test, encoding = encoding, drop = TRUE, verbose = TRUE)
```



```{r}
dx_$hypertension <- as.factor(dx_$hypertension)
dx_$heart_disease <- as.factor(dx_$heart_disease)
dx_$stroke <- as.factor(dx_$stroke)
dx_$gender <- as.factor(dx_$gender)
dx_$ever_married <- as.factor(dx_$ever_married)
dx_$work_type <- as.factor(dx_$work_type)
dx_$Residence_type <- as.factor(dx_$Residence_type)
dx_$smoking_status <- as.factor(dx_$smoking_status)
str(dx_)
```


#### Class Imbalance

```{r}
## Smote : Synthetic Minority Oversampling Technique To Handle Class Imbalancy In Binary Classification
# balanced.data <- SMOTE(Class ~., dresstrain, perc.over = 4800, k = 5, perc.under = 1000)
set.seed(3333)

balanced_dx_ <-SMOTE(stroke ~., dx_, perc.over = 2100 , k = 5, perc.under = 160)

tab_df(table(balanced_dx_$stroke), title = "Stroke Class")


```


### Cross Validation:
The first stage of validation is an internal validation in which we ensure that the developed CPM makes good predictions in ‘test’ data that comes from the same population as that in which the CPM was developed. By ‘good predictions’ we mean predictions that are well calibrated and have high discrimination.
Similar to the discussion in the previous module on regularised regression (lasso/ridge) a useful way of performing this is cross validation.
Recall that k-fold cross-validation involves:
Dividing the data we have available into k equally sized groups (commonly we use k between 5 and 10).
Fit the CPM using all the groups except one.
Use the remaining group as ‘test’ data to see how the model performs
Repeat this process k times, leaving out a different group each time.
Average performance over these k.
A simpler alternative to cross-validation would just be to divide the data into two groups, fit the data in one group and validate it in the other. You will commonly see this in the literature.
```{r}
# Random sample indexes
train_index <- sample(1:nrow(balanced_dx_), 0.8 * nrow(balanced_dx_))
test_index <- setdiff(1:nrow(balanced_dx_), train_index)

# Build X_train, y_train, X_test, y_test
X_train <- balanced_dx_[train_index, 2:12]
# y_train <- dx_[train_index, "stroke"]

X_test <- balanced_dx_[test_index, 2:12]
# y_test <- dx_[test_index, "stroke"]

table(X_train$stroke)
```


#### Model Training - Logistic Regression

```{r}
# balanced_dx_$hypertension <- as.factor(balanced_dx_$hypertension)
# balanced_dx_$heart_disease <- as.factor(balanced_dx_$heart_disease)
# balanced_dx_$stroke <- as.factor(balanced_dx_$stroke)
# balanced_dx_$gender <- as.factor(balanced_dx_$gender)
# str(balanced_dx_)
set.seed(3333)
model <- glm (stroke ~ ., data=X_train, family = binomial)
summary(model)
```
  


```{r}
stepAIC(model, direction='both')

model_select <- glm(formula = stroke ~ gender + age + hypertension + heart_disease + 
    ever_married + work_type + avg_glucose_level + smoking_status, 
    family = binomial, data = X_train)
summary(model_select)
```



```{r}
## Predict the Values
predict <- predict(model, X_test, type = 'response')

## Create Confusion Matrix
table(X_test$stroke, predict > 0.5)
```


Discrimination refers to the ability of a CPM to separate patients who will develop an outcome from those who will not. This is closely related to the concepts of sensitivity and specificity (as discussed in the last semester). A CPM that can have a simultaneously high sensitivity and specificity has good discrimination. However, the definition of sensitivity and specificity are a little limited for our purposes since they rely on a specific cutpoint. A receiver operator characteristic (ROC) curve plots sensitivity against ‘1-specificity’ across the full range of potential cutpoints.

```{r}
#ROCR Curve
ROCRpred <- prediction(predict, X_test$stroke)
ROCRperf <- performance(ROCRpred, 'tpr','fpr')
p <- plot(ROCRperf, colorize = TRUE, text.adj = c(-0.2,1.7))
abline(a=0, b=1)
```
The ‘ideal’ test (or CPM) has 100% sensitivity and 100% specificity – i.e. we know with certainty what the outcome is. A test (or CPM) with no value lies on the diagonal line – because I can build a CPM as good as this just by tossing a (weighted) coin for each patient to decide whether they have the outcome or not. So we expect that any reasonable CPM or test should fall within this ‘envelope’. The further to the top left of the graph the ROC curve falls, the better the test.


```{r}
auc(roc(X_test$stroke ~ predict))

hoslem.test(x = X_test$stroke, y=predict, g=10)
```
 We measure how far to the top left of the graph the curve is via the area under the curve (AUC):
So that an AUC of 1.0 represents perfect discrimination, an AUC of 0.5 represents no predictive value (no better than tossing a coin), and the AUC we would expect to see for our CPM is somewhere in between the two.
So that an AUC of 1.0 represents perfect discrimination, an AUC of 0.5 represents no predictive value (no better than tossing a coin), and the AUC we would expect to see for our CPM is somewhere in between the two.

Calibration and discrimination are two complementary measures, and in general we would like a CPM to perform well on both measures. Good calibration is generally the easier to achieve, but also ‘drifts’ most readily when the population changes. Poor calibration generally reflects a poor model rather than limitations in the data. Poor discrimination is hard to fix since this often reflects a lack of information (e.g. we do not know why, among a group of similar patients, some will go on to have a heart attack and some won’t) rather than problems in the actual model. 


#### Model Training - Random Forest 

```{r}
set.seed(3333)
rf = randomForest(stroke ~ ., 
                  ntree = 500,
                   data = X_train)
rf
plot(rf) 
```



```{r}
varImp(rf)
```



```{r}
## Important variables according to the model
varImpPlot(rf,  
           sort = T,
           n.var=10,
           main="Variable Importance")
```



```{r}
predicted.response <- predict(rf, X_test)


confusionMatrix(data=predicted.response,  
                reference=X_test$stroke)
```

#### Discrimination of Random Forest Classifier
Discrimination refers to the ability of a CPM to separate patients who will develop an outcome from those who will not. This is closely related to the concepts of sensitivity and specificity (as discussed in the last semester). A CPM that can have a simultaneously high sensitivity and specificity has good discrimination. However, the definition of sensitivity and specificity are a little limited for our purposes since they rely on a specific cutpoint. A receiver operator characteristic (ROC) curve plots sensitivity against ‘1-specificity’ across the full range of potential cutpoints.

```{r}
#ROCR Curve
ROCRpred_rf <- prediction(as.numeric(predicted.response), X_test$stroke)
ROCRperf_rf <- performance(ROCRpred_rf, 'tpr','fpr')
plot(ROCRperf_rf, colorize = TRUE, text.adj = c(-0.2,1.7))
abline(a=0, b=1)
```
The ‘ideal’ test (or CPM) has 100% sensitivity and 100% specificity – i.e. we know with certainty what the outcome is. A test (or CPM) with no value lies on the diagonal line – because I can build a CPM as good as this just by tossing a (weighted) coin for each patient to decide whether they have the outcome or not. So we expect that any reasonable CPM or test should fall within this ‘envelope’. The further to the top left of the graph the ROC curve falls, the better the test.


```{r}

auc(roc(X_test$stroke ~ as.numeric(predicted.response)))

hoslem.test(x = X_test$stroke, y=as.numeric(predicted.response), g=10)

```
 We measure how far to the top left of the graph the curve is via the area under the curve (AUC):
So that an AUC of 1.0 represents perfect discrimination, an AUC of 0.5 represents no predictive value (no better than tossing a coin), and the AUC we would expect to see for our CPM is somewhere in between the two.
So that an AUC of 1.0 represents perfect discrimination, an AUC of 0.5 represents no predictive value (no better than tossing a coin), and the AUC we would expect to see for our CPM is somewhere in between the two.

#### Calibration of Random Forest Classifier
Calibration and discrimination are two complementary measures, and in general we would like a CPM to perform well on both measures. Good calibration is generally the easier to achieve, but also ‘drifts’ most readily when the population changes. Poor calibration generally reflects a poor model rather than limitations in the data. Poor discrimination is hard to fix since this often reflects a lack of information (e.g. we do not know why, among a group of similar patients, some will go on to have a heart attack and some won’t) rather than problems in the actual model.


```{r}

```


### External validation
```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```